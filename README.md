# ğŸ§ª LLM Evaluation Experiments

This project explores how to evaluate Large Language Models (LLMs) using a variety of question-answering and commonsense reasoning tasks. It's intended as a hands-on prototype to understand how different LLMs (e.g., **Mistral**, **Phi-4**, **LLaMA**) perform under specific test prompts and evaluation methods.

The code and notebooks demonstrate:
- Running LLMs on test cases
- Logging outputs to a MongoDB database
- Retrieving results for post-hoc evaluation
- Calculating metrics like **accuracy**, **exact match**, and potentially **LLM-as-a-judge** scoring

> âš ï¸ **Note**: This is an exploratory, WIP (Work in Progress) project for learning and experimentation.

---

## ğŸ—‚ï¸ Project Structure

```
yashkushwaha-llm_evaluation/
â”œâ”€â”€ README.md                  # Project documentation
â”œâ”€â”€ pyproject.toml             # Poetry config (optional use)
â””â”€â”€ notebooks/
    â”œâ”€â”€ LLM runtime for QnA evaluation.ipynb
    â”œâ”€â”€ mistral evaluation.ipynb
    â”œâ”€â”€ Phi4 evaluation.ipynb
    â”œâ”€â”€ PIQA eval.ipynb
    â”œâ”€â”€ PIQA.ipynb
    â””â”€â”€ [Exported .html versions of the above notebooks]
```

---

## ğŸ“Œ Project Goals

- âš™ï¸ Test how different LLMs respond to structured input questions (e.g., QnA, PIQA)
- ğŸ§¾ Log generated answers to **MongoDB** for tracking and evaluation
- ğŸ“Š Evaluate outputs using metrics like:
  - Accuracy
  - Exact match (EM)
  - Optional: LLM-based scoring

---

## ğŸ”§ Technologies Used

- **Jupyter Notebooks** for runtime and evaluation logic
- **Python** with Hugging Face or API-based LLM calls (assumed)
- **MongoDB** to persist model outputs
- **Poetry** for dependency management (optional)

---

## ğŸš€ How to Use

### 1. Run LLM Inference

Use the notebook:

```
notebooks/LLM runtime for QnA evaluation.ipynb
```

- Prompts are sent to the model (e.g., Mistral, Phi-4)
- Responses are saved to a MongoDB collection for later analysis

### 2. Evaluate Outputs

Run the evaluation notebooks, such as:

```
notebooks/mistral evaluation.ipynb
notebooks/Phi4 evaluation.ipynb
notebooks/PIQA eval.ipynb
```

- These load response data from MongoDB
- Compute metrics to assess model accuracy and behavior

---

## ğŸ§  Observations & Future Work

This is an evolving project. Possible enhancements include:

- Unifying runtime and evaluation via a shared library or script
- Adding support for multiple metrics (BLEU, ROUGE, BERTScore, etc.)
- Comparing zero-shot vs. few-shot performance
- Automating eval loops with LLM-as-a-judge methods

---

## ğŸ“š References

- [OpenAI / Anthropic / Hugging Face APIs]
- [MongoDB Python Docs](https://pymongo.readthedocs.io/)
- [PIQA dataset](https://paperswithcode.com/dataset/piqa)
- [BoolQ dataset](https://paperswithcode.com/dataset/boolq)

---
